---
title: "VT0007 Jurisdictional Deforestation Risk Maps"
date: 2024-11-04
author: 
  - name: Seamus Murphy
    orcid: 0000-0002-1792-0351 
    email: seamusrobertmurphy@gmail.com
    degrees:
      - PhD
abstract: > 
  The following workflow provides a starter script for deriving deforestation 
  risk maps in accordance with Verra's new methodology for unplanned 
  deforestation allocation in jurisdictional nested REDD+ projects using the 
  [VT0007 toolset](https://verra.org/wp-content/uploads/2024/02/VT0007-Unplanned-Deforestation-Allocation-v1.0.pdf).
keywords:
  - REDD+
  - VCS
  - Verra
  - Carbon verification
  - Jurisdictional
format: 
  html:
    toc: true
    toc-location: right
    toc-title: "**Contents**"
    toc-depth: 5
    toc-expand: 4
    theme: [minimal, styles.scss]
highlight-style: github
df-print: kable
keep-md: true
prefer-html: true
bibliography: references.bib
---

```{r setup}
#| warning: false
#| message: false
#| error: false
#| include: false
#| echo: false

#install.packages("easypackages")
easypackages::packages(
  "animation",
  "BIOMASS",
  "caret", 
  "cli", 
  "cols4all", 
  "covr", 
  "cowplot",
  "dendextend", 
  "digest",
  "DiagrammeR",
  "dplyr",
  "dtwclust", 
  "e1071", 
  "exactextractr", 
  "extrafont",
  "FNN", 
  "future",
  "gdalcubes", 
  "gdalUtilities", 
  "geojsonsf",
  "googledrive",
  "hdf5r", 
  "htmltools",
  "httr", 
  "httr2",
  "jsonlite", 
  "kableExtra",
  "knitr",
  "kohonen", 
  "leafem", 
  "libgeos",
  "lintr",
  "luz",
  "mapedit", 
  "mapview", 
  "maptiles", 
  "methods",
  "mgcv", 
  "ncdf4", 
  "nnet", 
  "openxlsx", 
  "palette",
  "parallel",
  "randomForest", 
  "rasterVis", 
  "raster", 
  "Rcpp", 
  "RcppArmadillo", 
  "RcppCensSpatial", 
  "RcppEigen", 
  "RcppParallel", 
  "RColorBrewer", 
  "reticulate",
  "rgee",
  "rgeeExtra",
  "rsconnect",
  "RStoolbox", 
  "rts", 
  "sf", 
  "scales", 
  "sits",
  "spdep", 
  "stars", 
  "stringr",
  "sits",
  "sitsdata",
  "styler",
  "supercells", 
  "terra", 
  "testthat", 
  "tidyverse",
  "tinytex",
  "tmap", 
  "tmaptools",
  "xgboost"
  )

knitr::opts_chunk$set(
  echo        = TRUE, 
  message     = FALSE, 
  warning     = FALSE, 
  error       = TRUE, 
  comment     = NA, 
  tidy.opts   = list(width.cutoff = 60)
) 

base::options(
  htmltools.dir.version = FALSE, 
  htmltools.preserve.raw = FALSE)

# switch on/off for spherical geometries
#sf::sf_use_s2(use_s2 = TRUE)
sf::sf_use_s2(use_s2 = FALSE)
```

## Summary

Two workflow approaches are detailed below.
Workflow-2 is coded using Python and Google Earth Engine functions that are more suited to larger areas of interest.
For comparison purposes, both workflows derive outputs from the same image collection of STAC-formatted analysis-ready-data of Landsat scenes, following steps outlined in Verra's recommended sequence of deforestation risk map development @verraVT0007UnplannedDeforestation2021 .
Workflow-1, which is coded using the R ecosystem, allows additional model tuning functions suited to analysis of smaller areas.

![Figure 1: Verra's recommended risk map development sequence (VT0007:6)](VT0007-risk-map-development-sequence.png)

# 1. Workflow in R -\> `sits`

## Process data cube

For shawcasing purposes, we import a training dataset from the `sitsdata` package from a study of Brazil's Samuel Hydroelectric Dam in Rondonia State conducted between 2020-06-04 to 2021-08-26.
To evaluate this training sample, we assemble below a data cube of Sentinel-2-L2A-COGS images from the AWS open bucket.
Raster normalization is implemented with `sits_regularize` functions to apply a cloud masking and back-filling of missing pixels by cloudless ranking and global median values across 16-day intervals.

```{r}
# build irregular data cube from single sentinel tile
s2_cube_ro <- sits_cube(
  source = "AWS",
  collection = "SENTINEL-S2-L2A-COGS",
  tiles = "20LMR",
  bands = c("B02", "B8A", "B11", "SCL"),
  start_date = as.Date("2020-06-01"),
  end_date = as.Date("2021-09-01"),
  progress = FALSE)

# select aoi inside the tile
roi <- c(
  lon_max = -63.25790, lon_min = -63.6078,
  lat_max = -8.72290, lat_min = -8.95630)

# regularize the aoi filtered cube
s2_reg_cube_ro <- sits_regularize(
  cube = s2_cube_ro,
  output_dir = "./cubes/01_reg",
  res = 30,
  roi = roi,
  period = "P16D",
  memsize = 16,
  multicores = 4,
  progress = FALSE)
```

## Review data cube

```{r}
#| layout-ncol: 3

plot(s2_reg_cube_ro,
  red = "B11",
  green = "B8A",
  blue = "B02",
  date = "2020-07-04"
  )

plot(s2_reg_cube_ro,
  red = "B11",
  green = "B8A",
  blue = "B02",
  date = "2020-11-09"
  )

plot(s2_reg_cube_ro, 
     red = "B11", 
     green = "B8A", 
     blue = "B02", 
     date = "2021-08-08"
     )
```

## Classify data cube

We import a training set of 480 times series points specifically designed to detect deforestation, which comprise of four classes (`Burned_Area`, `Forest`, `Highly_Degraded`, and `Cleared_Area`).
Training samples are fitted to a Random Forest model and post-processed with a Bayesian smoothing.

```{r}
# Load the training set
data("samples_prodes_4classes")
# Select the same three bands used in the data cube
samples_4classes_3bands <- sits_select(
  data = samples_prodes_4classes,
  bands = c("B02", "B8A", "B11")
  )

# Train a random forest model
rfor_model <- sits_train(
  samples = samples_4classes_3bands,
  ml_method = sits_rfor()
  )

# Classify the small area cube
s2_cube_probs <- sits_classify(
  data = s2_reg_cube_ro,
  ml_model = rfor_model,
  output_dir = "./cubes/02_class/",
  memsize = 15,
  multicores = 5
  )

# Post-process the probability cube
s2_cube_bayes <- sits_smooth(
  cube = s2_cube_probs,
  output_dir = "./cubes/02_class/",
  memsize = 16,
  multicores = 4
  )

# Label the post-processed  probability cube
s2_cube_label <- sits_label_classification(
  cube = s2_cube_bayes,
  output_dir = "./cubes/02_class/",
  memsize = 16,
  multicores = 4
  )

plot(s2_cube_label)
```

## Map uncertainty

To improve model performance, we estimate class uncertainty and plot these pixel error metrics.
Results below reveal highest uncertainty levels in classification of wetland and water areas.

```{r}
# Calculate the uncertainty cube
s2_cube_uncert <- sits_uncertainty(
  cube = s2_cube_bayes,
  type = "margin",
  output_dir = "./cubes/03_error/",
  memsize = 16,
  multicores = 4
)

plot(s2_cube_uncert)
```

As expected, the places of highest uncertainty are those covered by surface water or associated with wetlands.
These places are likely to be misclassified.
For this reason, sits provides `sits_uncertainty_sampling()`, which takes the uncertainty cube as its input and produces a tibble with locations in WGS84 with high uncertainty [@camaraUncertaintyActiveLearning].

```{r}
# Find samples with high uncertainty
new_samples <- sits_uncertainty_sampling(
  uncert_cube = s2_cube_uncert,
  n = 20,
  min_uncert = 0.5,
  sampling_window = 10
  )

# View the location of the samples
sits_view(new_samples)
```

## Add training samples

We can then use these points of high-uncertainty as new samples to add to our current training dataset.
Once we identify their feature classes and relabel them correctly, we append them to derive an augmented `samples_round_2`.

```{r}
# Label the new samples
new_samples$label <- "Wetland"

# Obtain the time series from the regularized cube
new_samples_ts <- sits_get_data(
  cube = s2_reg_cube_ro,
  samples = new_samples
  )

# Add new class to original samples
samples_round_2 <- dplyr::bind_rows(
  samples_4classes_3bands,
  new_samples_ts
  )

# Train a RF model with the new sample set
rfor_model_v2 <- sits_train(
  samples = samples_round_2,
  ml_method = sits_rfor()
  )

# Classify the small area cube
s2_cube_probs_v2 <- sits_classify(
  data = s2_reg_cube_ro,
  ml_model = rfor_model_v2,
  output_dir = "./cubes/02_class/",
  version = "v2",
  memsize = 16,
  multicores = 4
  )

# Post-process the probability cube
s2_cube_bayes_v2 <- sits_smooth(
  cube = s2_cube_probs_v2,
  output_dir = "./cubes/04_smooth/",
  version = "v2",
  memsize = 16,
  multicores = 4
  )

# Label the post-processed  probability cube
s2_cube_label_v2 <- sits_label_classification(
  cube = s2_cube_bayes_v2,
  output_dir = "./cubes/05_tuned/",
  version = "v2",
  memsize = 16,
  multicores = 4
  )

# Plot the second version of the classified cube
plot(s2_cube_label_v2)
```

## Remap uncertainty

```{r}
# Calculate the uncertainty cube
s2_cube_uncert_v2 <- sits_uncertainty(
  cube = s2_cube_bayes_v2,
  type = "margin",
  output_dir = "./cubes/03_error/",
  version = "v2",
  memsize = 16,
  multicores = 4
)

plot(s2_cube_uncert_v2)
```

## Accuracy assessment

To select a validation subset of the map, `sits` recommends Cochranâ€™s method for stratified random sampling [@cochran1977sampling].
The method divides the population into homogeneous subgroups, or strata, and then applying random sampling within each stratum.
Alternatively, ad-hoc parameterization is suggested as follows.

```{r}
ro_sampling_design <- sits_sampling_design(
  cube = s2_cube_label_v2,
  expected_ua = c(
    "Burned_Area"       = 0.75,
    "Cleared_Area"      = 0.70,
    "Forest"            = 0.75,
    "Highly_Degraded"   = 0.70,
    "Wetland"           = 0.70
  ),
  alloc_options         = c(120, 100),
  std_err               = 0.01,
  rare_class_prop       = 0.1
)
# show sampling desing
ro_sampling_design
```

## Split train/test data

```{r}
ro_samples_sf <- sits_stratified_sampling(
  cube                  = s2_cube_label_v2,
  sampling_design       = ro_sampling_design,
  alloc                 = "alloc_120",
  multicores            = 4,
  shp_file              = "./samples/ro_samples.shp"
)

sf::st_write(ro_samples_sf,
  "./samples/ro_samples.csv",
  layer_options = "GEOMETRY=AS_XY",
  append = FALSE # TRUE if editing existing sample
)
```

## Confusion matrix

```{r}
# Calculate accuracy according to Olofsson's method
area_acc <- sits_accuracy(s2_cube_label_v2,
  validation = ro_samples_sf,
  multicores = 4
)
# Print the area estimated accuracy
area_acc

# Print the confusion matrix
area_acc$error_matrix
```

## Times series visualization

```{r}
summary(as.data.frame(ro_samples_sf))
```

## Deforestation binary map

## Deforestation risk map

# 2. Workflow in Python -\> `GEE`

```{r}
#| eval: false
# Set your Python ENV
Sys.setenv("RETICULATE_PYTHON" = "/usr/bin/python3")
# Set Google Cloud SDK 
Sys.setenv("EARTHENGINE_GCLOUD" = "~/seamus/google-cloud-sdk/bin/")

library(rgee) 
ee_Authenticate()
ee_install_upgrade()
ee_Initialize()
```

#### Housekeeping

```{r}
#| eval: true
# convert markdown to script.R 
knitr::purl("VT0007-deforestation-risk-map.qmd")

# display environment setup
devtools::session_info()
```
