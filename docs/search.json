[
  {
    "objectID": "VT0007-deforestation-map.html",
    "href": "VT0007-deforestation-map.html",
    "title": "JNR Deforestation Risk Maps",
    "section": "",
    "text": "The following details a possible workflow approach to Verra’s recommended sequence of deforestation risk map development [Verra (2021); Figure 1] . Workflow inputs include a filtered subset of the global training sample data developed by(Stanimirova et al. 2023) and the imagery from the Landsat Collection-2 Level-2 Tier-1 processed rasters.\n\n\n\nFigure 1: Verra’s recommended risk map development sequence (VT0007:6)\n\n\n\n\nBuild/restore virtual environment: Python -&gt; R\nTo avoid issues with IDE settings, it is recommended to run the following virtual environment functions from an terminal external to RStudio or VScode. To update an previously loaded environment, simply run pip3 install -r requirements.txt in any terminal from the trunk directory.\n\n# create virtual environment \npython3 -m venv working_director_name\n\n# activate environment's python\nsource working_director_name/bin/activate\n\n# check python activation\npython3\nimport sys \nprint(sys.executable)\nquit()\n\n# restore environment of cloned repo\npython3 pip install -r requirements.txt\n\n# install packages manually\npython3 -m pip install numpy jupyter earthengine-api\n\n# save index of loaded packages\npython3 -m pip freeze &gt; requirements.txt\n\nAssign rgee kernel, gcs directory & credentials\n\nlibrary(rgee)\nlibrary(reticulate)\nlibrary(googledrive)\nlibrary(googleCloudStorageR)\n\nreticulate::use_python(\"./bin/python3\")\nreticulate::py_run_string(\"import ee; ee.Initialize()\")\nrgee::ee_install_set_pyenv(py_path = \"./bin/python3\", py_env = \"./\")\nrgee::ee_path = path.expand(\"/home/seamus/.config/earthengine/seamusrobertmurphy/credentials\")\nee_Initialize(user = \"seamusrobertmurphy\", gcs = T, drive = T)\n#ee_install()\n\n################################################\n# Assign the SaK & user for interactive web renders\nSaK_file = \"/home/seamus/Repos/api-keys/SaK_rgee.json\" \nee_utils_sak_copy(sakfile =  SaK_file, users = \"seamusrobertmurphy\")\n\n# Confirm project_id & bucket\nproject_id &lt;- ee_get_earthengine_path() %&gt;% \n  list.files(., \"\\\\.json$\", full.names = TRUE) %&gt;% \n  jsonlite::read_json() %&gt;% \n  '$'(project_id) \n#googleCloudStorageR::gcs_create_bucket(\"deforisk_bucket_1\", projectId = project_id)\n\n# Validate SaK credentials\nee_utils_sak_validate(\n    sakfile = SaK_file,\n    bucket = \"deforisk_bucket_1\",\n    quiet = F \n )\n\nJurisdictional boundaries\n\n# assign master crs\ncrs_master    = sf::st_crs(\"epsg:4326\")\n# derive aoi windows\naoi_country   = geodata::gadm(country=\"GUY\", level=0, path=tempdir()) |&gt;\n  sf::st_as_sf() |&gt; sf::st_cast() |&gt; sf::st_transform(crs_master)\n\naoi_states    = geodata::gadm(country=\"GUY\", level=1, path=tempdir()) |&gt;\n  sf::st_as_sf() |&gt; sf::st_cast() |&gt; sf::st_transform(crs_master) |&gt;\n  dplyr::rename(State = NAME_1)\n\naoi_target    = dplyr::filter(aoi_states, State == \"Barima-Waini\") \naoi_target_ee = rgee::sf_as_ee(aoi_target)\n\nEarth Engine client library not initialized. Run `ee.Initialize()`\n\n# visualize\ntmap::tmap_mode(\"view\")\ntmap::tm_shape(aoi_states) + tmap::tm_borders(col = \"white\", lwd = 0.5) +\n  tmap::tm_text(\"State\", col = \"white\", size = 1, alpha = 0.3, just = \"bottom\") +\n  tmap::tm_shape(aoi_country) + tmap::tm_borders(col = \"white\", lwd = 1) +\n  tmap::tm_shape(aoi_target) + tmap::tm_borders(col = \"red\", lwd = 2) +\n  tmap::tm_text(\"State\", col = \"red\", size = 1.3) +\n  tmap::tm_basemap(\"Esri.WorldImagery\")\n\n\n\n\n\n\n\n\nWe assemble a raster data cube representing a ten year historical reference period (HRP) between 2014-01-01 and 2024-12-31 for the state of Barina Waini, Guyana. Masking is applied to cloud, shadow and water surfaces with median normalization using a cloudless pixel ranking.\n\ncube_terra = terra::rast(\"./cubes/mosaic/LANDSAT-C2-L2_OLI_TILE_BAND_2014-01-11.tif\")\nnames(cube_terra)\n\ncube_2014 = sits_cube(\n  source     = \"MPC\",\n  collection = \"LANDSAT-C2-L2\",\n  data_dir   = here::here(\"cubes\", \"mosaic\"),\n  bands      = c(\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B10\", \"NDVI\"),\n  version    = \"mosaic\"\n)\n\n# 2014 -------------------\n# cloud-assemble data cube\ncube_raw_2014 = sits::sits_cube(\n  source      = \"MPC\",\n  collection  = \"LANDSAT-C2-L2\",\n  bands       = c(\"RED\", \"GREEN\", \"BLUE\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n  roi         = aoi_target,\n  start_date  = as.Date(\"2014-01-01\"),\n  end_date    = as.Date(\"2014-01-15\"),\n  progress    = T\n  )\n\n# regularize data cube\ncube_reg_2014 = sits::sits_regularize(\n  cube        = cube_raw_2014,\n  roi         = aoi_target,\n  res         = 60,\n  period      = \"P180D\",\n  output_dir  = here::here(\"cubes\", \"reg\", \"2014\"),\n  memsize     = 16,\n  multicores  = 8,\n  progress    = T\n  )\n\n# 2019 -------------------\n# cloud-assemble data cube\ncube_raw_2019 = sits::sits_cube(\n  source      = \"MPC\",\n  collection  = \"LANDSAT-C2-L2\",\n  bands       = c(\"RED\", \"GREEN\", \"BLUE\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n  roi         = aoi_target,\n  start_date  = as.Date(\"2019-01-01\"),\n  end_date    = as.Date(\"2019-07-01\"),\n  progress    = T\n  )\n\n# regularize data cube\ncube_reg_2019 = sits::sits_regularize(\n  cube        = cube_raw_2019,\n  roi         = aoi_target,\n  res         = 60,\n  period      = \"P180D\",\n  output_dir  = here::here(\"cubes\", \"reg\", \"2019\"),\n  memsize     = 16,\n  multicores  = 8,\n  progress    = T\n  )\n\n# 2024 -------------------\n# cloud-assemble data cube\ncube_raw_2024 = sits::sits_cube(\n  source      = \"MPC\",\n  collection  = \"LANDSAT-C2-L2\",\n  bands       = c(\"RED\", \"GREEN\", \"BLUE\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n  roi         = aoi_target,\n  start_date  = as.Date(\"2024-01-01\"),\n  end_date    = as.Date(\"2024-07-01\"),\n  progress    = T\n  )\n\n# regularize data cube\ncube_reg_2024 = sits::sits_regularize(\n  cube        = cube_raw_2024,\n  roi         = aoi_target,\n  res         = 60,\n  period      = \"P180D\",\n  output_dir  = here::here(\"cubes\", \"reg\", \"2024\"),\n  memsize     = 16,\n  multicores  = 8,\n  progress    = T\n  )\n\n# plot cube timelines\nsits_timeline(cube_reg_2014)\nsits_timeline(cube_reg_2019)\nsits_timeline(cube_reg_2024)\nplot(cube_reg_2014,\n  red         = \"RED\",\n  green       = \"GREEN\",\n  blue        = \"BLUE\",\n  date        = \"2014-01-03\"\n  )\n\nplot(cube_reg_2019,\n  red         = \"RED\",\n  green       = \"GREEN\",\n  blue        = \"BLUE\",\n  date        = \"2019-01-08\"\n  )\n\nplot(cube_reg_2024,\n  red         = \"RED\",\n  green       = \"GREEN\",\n  blue        = \"BLUE\",\n  date        = \"2024-01-07\"\n  )\n\n\n\n\n\nWe import the GLanCE training dataset of annual times series points that includes 7 land cover classes (Figure 2; (Woodcock et al., n.d.)). Training samples are fitted to a Random Forest model and post-processed with a Bayesian smoothing and then evaluated using confusion matrix. The classifier is then calibrated by mapping pixel uncertainty, adding new samples in areas of high uncertainty, reclassifying with improved samples and re-evaluated using confusion matrix.\n\n\n\nFigure 2: Land cover classes included in the GLanCE Level 1 classification scheme (Woodcock et al 2022)\n\n\n\n# Extract training set from gee to drive & import: https://gee-community-catalog.org/projects/glance_training/?h=training \nglance_training_url = \"https://drive.google.com/file/d/1CgBP2J2OdOhmOiVS4hGibLEMyVLTe1_P/view?usp=drive_link\"\n# file_name = \"glance_training.csv\"\n# download.file(url = url, path = here::here(\"training\"), destfile = file_name)\nglance_training = read.csv(here::here(\"training\", \"glance_training.csv\"))\n\ndata(\"samples_prodes_4classes\")\n# Select the same three bands used in the data cube\nsamples_4classes_3bands &lt;- sits_select(\n  data = samples_prodes_4classes,\n  bands = c(\"B02\", \"B8A\", \"B11\")\n  )\n\n# Train a random forest model\nrfor_model &lt;- sits_train(\n  samples = samples_4classes_3bands,\n  ml_method = sits_rfor()\n  )\n\n# Classify the small area cube\ns2_cube_probs &lt;- sits_classify(\n  data = s2_reg_cube_ro,\n  ml_model = rfor_model,\n  output_dir = \"./cubes/02_class/\",\n  memsize = 15,\n  multicores = 5\n  )\n\n# Post-process the probability cube\ns2_cube_bayes &lt;- sits_smooth(\n  cube = s2_cube_probs,\n  output_dir = \"./cubes/02_class/\",\n  memsize = 16,\n  multicores = 4\n  )\n\n# Label the post-processed  probability cube\ns2_cube_label &lt;- sits_label_classification(\n  cube = s2_cube_bayes,\n  output_dir = \"./cubes/02_class/\",\n  memsize = 16,\n  multicores = 4\n  )\n\nplot(s2_cube_label)\n\n\n\n\n\nTo improve model performance, we estimate class uncertainty and plot these pixel error metrics. Results below reveal highest uncertainty levels in classification of wetland and water areas.\n\n# Calculate the uncertainty cube\ns2_cube_uncert &lt;- sits_uncertainty(\n  cube = s2_cube_bayes,\n  type = \"margin\",\n  output_dir = \"./cubes/03_error/\",\n  memsize = 16,\n  multicores = 4\n)\n\nplot(s2_cube_uncert)\n\nAs expected, the places of highest uncertainty are those covered by surface water or associated with wetlands. These places are likely to be misclassified. For this reason, sits provides sits_uncertainty_sampling(), which takes the uncertainty cube as its input and produces a tibble with locations in WGS84 with high uncertainty (Camara et al., n.d.).\n\n# Find samples with high uncertainty\nnew_samples &lt;- sits_uncertainty_sampling(\n  uncert_cube = s2_cube_uncert,\n  n = 20,\n  min_uncert = 0.5,\n  sampling_window = 10\n  )\n\n# View the location of the samples\nsits_view(new_samples)\n\n\n\n\nWe can then use these points of high-uncertainty as new samples to add to our current training dataset. Once we identify their feature classes and relabel them correctly, we append them to derive an augmented samples_round_2.\n\n# Label the new samples\nnew_samples$label &lt;- \"Wetland\"\n\n# Obtain the time series from the regularized cube\nnew_samples_ts &lt;- sits_get_data(\n  cube = s2_reg_cube_ro,\n  samples = new_samples\n  )\n\n# Add new class to original samples\nsamples_round_2 &lt;- dplyr::bind_rows(\n  samples_4classes_3bands,\n  new_samples_ts\n  )\n\n# Train a RF model with the new sample set\nrfor_model_v2 &lt;- sits_train(\n  samples = samples_round_2,\n  ml_method = sits_rfor()\n  )\n\n# Classify the small area cube\ns2_cube_probs_v2 &lt;- sits_classify(\n  data = s2_reg_cube_ro,\n  ml_model = rfor_model_v2,\n  output_dir = \"./cubes/02_class/\",\n  version = \"v2\",\n  memsize = 16,\n  multicores = 4\n  )\n\n# Post-process the probability cube\ns2_cube_bayes_v2 &lt;- sits_smooth(\n  cube = s2_cube_probs_v2,\n  output_dir = \"./cubes/04_smooth/\",\n  version = \"v2\",\n  memsize = 16,\n  multicores = 4\n  )\n\n# Label the post-processed  probability cube\ns2_cube_label_v2 &lt;- sits_label_classification(\n  cube = s2_cube_bayes_v2,\n  output_dir = \"./cubes/05_tuned/\",\n  version = \"v2\",\n  memsize = 16,\n  multicores = 4\n  )\n\n# Plot the second version of the classified cube\nplot(s2_cube_label_v2)\n\n\n\n\n\n# Calculate the uncertainty cube\ns2_cube_uncert_v2 &lt;- sits_uncertainty(\n  cube = s2_cube_bayes_v2,\n  type = \"margin\",\n  output_dir = \"./cubes/03_error/\",\n  version = \"v2\",\n  memsize = 16,\n  multicores = 4\n)\n\nplot(s2_cube_uncert_v2)\n\n\n\n\nTo select a validation subset of the map, sits recommends Cochran’s method for stratified random sampling (Cochran 1977). The method divides the population into homogeneous subgroups, or strata, and then applying random sampling within each stratum. Alternatively, ad-hoc parameterization is suggested as follows.\n\nro_sampling_design &lt;- sits_sampling_design(\n  cube = s2_cube_label_v2,\n  expected_ua = c(\n    \"Burned_Area\"       = 0.75,\n    \"Cleared_Area\"      = 0.70,\n    \"Forest\"            = 0.75,\n    \"Highly_Degraded\"   = 0.70,\n    \"Wetland\"           = 0.70\n  ),\n  alloc_options         = c(120, 100),\n  std_err               = 0.01,\n  rare_class_prop       = 0.1\n)\n# show sampling desing\nro_sampling_design\n\n\n\n\n\nro_samples_sf &lt;- sits_stratified_sampling(\n  cube                  = s2_cube_label_v2,\n  sampling_design       = ro_sampling_design,\n  alloc                 = \"alloc_120\",\n  multicores            = 4,\n  shp_file              = \"./samples/ro_samples.shp\"\n)\n\nsf::st_write(ro_samples_sf,\n  \"./samples/ro_samples.csv\",\n  layer_options = \"GEOMETRY=AS_XY\",\n  append = FALSE # TRUE if editing existing sample\n)\n\n\n\n\n\n# Calculate accuracy according to Olofsson's method\narea_acc &lt;- sits_accuracy(s2_cube_label_v2,\n  validation = ro_samples_sf,\n  multicores = 4\n)\n# Print the area estimated accuracy\narea_acc\n\n# Print the confusion matrix\narea_acc$error_matrix\n\n\n\n\n\nsummary(as.data.frame(ro_samples_sf))"
  },
  {
    "objectID": "VT0007-deforestation-map.html#summary",
    "href": "VT0007-deforestation-map.html#summary",
    "title": "JNR Deforestation Risk Maps",
    "section": "",
    "text": "The following details a workflow approach to Verra’s recommended sequence of deforestation risk map development Verra (2021). Workflow inputs include a filtered subset of the global training sample data developed by(Stanimirova et al. 2023) and the imagery from the Landsat Collection-2 Level-2 Tier-1 processed rasters.\n\n\n\nFigure 1: Verra’s recommended risk map development sequence (VT0007:6)"
  },
  {
    "objectID": "VT0007-deforestation-map.html#testing-phase",
    "href": "VT0007-deforestation-map.html#testing-phase",
    "title": "JNR Deforestation Risk Maps",
    "section": "1.1 Testing phase",
    "text": "1.1 Testing phase\n\nimport numpy as np\nimport sys \nprint(sys.executable)\n\n/home/seamus/Repos/VT0007-deforestation-map/bin/python\n\n\n\nJurisdictional boundaries\n\n# assign master crs\ncrs_master  = sf::st_crs(\"epsg:4326\")\naoi_country = geodata::gadm(country=\"GUY\", level=0, path=tempdir()) |&gt;\n  sf::st_as_sf() |&gt; sf::st_cast() |&gt; sf::st_transform(crs_master)\naoi_states = geodata::gadm(country=\"GUY\", level=1, path=tempdir()) |&gt;\n  sf::st_as_sf() |&gt; sf::st_cast() |&gt; sf::st_transform(crs_master) |&gt;\n  dplyr::rename(State = NAME_1)\naoi_target = dplyr::filter(aoi_states, State == \"Barima-Waini\")\n\ntmap::tmap_mode(\"view\")\ntmap::tm_shape(aoi_states) + tmap::tm_borders(col = \"white\", lwd = 0.5) +\n  tmap::tm_text(\"State\", col = \"white\", size = 1, alpha = 0.3, just = \"bottom\") +\n  tmap::tm_shape(aoi_country) + tmap::tm_borders(col = \"white\", lwd = 1) +\n  tmap::tm_shape(aoi_target) + tmap::tm_borders(col = \"red\", lwd = 2) +\n  tmap::tm_text(\"State\", col = \"red\", size = 1.3) +\n  tmap::tm_basemap(\"Esri.WorldImagery\")\n\n\n\n\n\n\n\nAssemble HRP time series\nWe assemble and process a data cube representing a historical reference period (HRP) between 2013-01-01 and 2023-01-01 for the country of Suriname. Using the sits_regularize functions, we apply a cloud masking and pixel back-filling based on cloudless ranking and median-normalization across 5-year intervals to derive three dry-season mosaics for 2013, 2018 and 2023.\n\n# 2014 -------------------\n# cloud-assemble data cube\ncube_raw_2014 = sits::sits_cube(\n  source      = \"MPC\",\n  collection  = \"LANDSAT-C2-L2\",\n  bands       = c(\"RED\", \"GREEN\", \"BLUE\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n  roi         = aoi_target,\n  start_date  = as.Date(\"2014-01-01\"),\n  end_date    = as.Date(\"2014-07-01\"),\n  progress    = T\n  )\n\n# regularize data cube\ncube_reg_2014 = sits::sits_regularize(\n  cube        = cube_raw_2014,\n  roi         = aoi_target,\n  res         = 60,\n  period      = \"P180D\",\n  output_dir  = here::here(\"cubes\", \"reg\", \"2014\"),\n  memsize     = 16,\n  multicores  = 8,\n  progress    = T\n  )\n\n# 2019 -------------------\n# cloud-assemble data cube\ncube_raw_2019 = sits::sits_cube(\n  source      = \"MPC\",\n  collection  = \"LANDSAT-C2-L2\",\n  bands       = c(\"RED\", \"GREEN\", \"BLUE\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n  roi         = aoi_target,\n  start_date  = as.Date(\"2019-01-01\"),\n  end_date    = as.Date(\"2019-07-01\"),\n  progress    = T\n  )\n\n# regularize data cube\ncube_reg_2019 = sits::sits_regularize(\n  cube        = cube_raw_2019,\n  roi         = aoi_target,\n  res         = 60,\n  period      = \"P180D\",\n  output_dir  = here::here(\"cubes\", \"reg\", \"2019\"),\n  memsize     = 16,\n  multicores  = 8,\n  progress    = T\n  )\n\n# 2024 -------------------\n# cloud-assemble data cube\ncube_raw_2024 = sits::sits_cube(\n  source      = \"MPC\",\n  collection  = \"LANDSAT-C2-L2\",\n  bands       = c(\"RED\", \"GREEN\", \"BLUE\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n  roi         = aoi_target,\n  start_date  = as.Date(\"2024-01-01\"),\n  end_date    = as.Date(\"2024-07-01\"),\n  progress    = T\n  )\n\n# regularize data cube\ncube_reg_2024 = sits::sits_regularize(\n  cube        = cube_raw_2024,\n  roi         = aoi_target,\n  res         = 60,\n  period      = \"P180D\",\n  output_dir  = here::here(\"cubes\", \"reg\", \"2024\"),\n  memsize     = 16,\n  multicores  = 8,\n  progress    = T\n  )\n\n# plot cube timelines\nsits_timeline(cube_reg_2014)\nsits_timeline(cube_reg_2019)\nsits_timeline(cube_reg_2024)\nplot(cube_reg_2014,\n  red         = \"RED\",\n  green       = \"GREEN\",\n  blue        = \"BLUE\",\n  date        = \"2014-01-03\"\n  )\n\nplot(cube_reg_2019,\n  red         = \"RED\",\n  green       = \"GREEN\",\n  blue        = \"BLUE\",\n  date        = \"2019-01-08\"\n  )\n\nplot(cube_reg_2024,\n  red         = \"RED\",\n  green       = \"GREEN\",\n  blue        = \"BLUE\",\n  date        = \"2024-01-07\"\n  )\n\n\n\n\n\nClassify HRP time series\nWe import the GLanCE training dataset of annual times series points that includes 7 land cover classes (Figure 2; (Woodcock et al., n.d.)). Training samples are fitted to a Random Forest model and post-processed with a Bayesian smoothing and then evaluated using confusion matrix. The classifier is then calibrated by mapping pixel uncertainty, adding new samples in areas of high uncertainty, reclassifying with improved samples and re-evaluated using confusion matrix.\n\n\n\nFigure 2: Land cover classes included in the GLanCE Level 1 classification scheme (Woodcock et al 2022)\n\n\n\n# Extract training set from gee to drive & import: https://gee-community-catalog.org/projects/glance_training/?h=training \nglance_training_url = \"https://drive.google.com/file/d/1CgBP2J2OdOhmOiVS4hGibLEMyVLTe1_P/view?usp=drive_link\"\n# file_name = \"glance_training.csv\"\n# download.file(url = url, path = here::here(\"training\"), destfile = file_name)\nglance_training = read.csv(here::here(\"training\", \"glance_training.csv\"))\n\ndata(\"samples_prodes_4classes\")\n# Select the same three bands used in the data cube\nsamples_4classes_3bands &lt;- sits_select(\n  data = samples_prodes_4classes,\n  bands = c(\"B02\", \"B8A\", \"B11\")\n  )\n\n# Train a random forest model\nrfor_model &lt;- sits_train(\n  samples = samples_4classes_3bands,\n  ml_method = sits_rfor()\n  )\n\n# Classify the small area cube\ns2_cube_probs &lt;- sits_classify(\n  data = s2_reg_cube_ro,\n  ml_model = rfor_model,\n  output_dir = \"./cubes/02_class/\",\n  memsize = 15,\n  multicores = 5\n  )\n\n# Post-process the probability cube\ns2_cube_bayes &lt;- sits_smooth(\n  cube = s2_cube_probs,\n  output_dir = \"./cubes/02_class/\",\n  memsize = 16,\n  multicores = 4\n  )\n\n# Label the post-processed  probability cube\ns2_cube_label &lt;- sits_label_classification(\n  cube = s2_cube_bayes,\n  output_dir = \"./cubes/02_class/\",\n  memsize = 16,\n  multicores = 4\n  )\n\nplot(s2_cube_label)"
  },
  {
    "objectID": "VT0007-deforestation-map.html#map-uncertainty",
    "href": "VT0007-deforestation-map.html#map-uncertainty",
    "title": "JNR Deforestation Risk Maps",
    "section": "",
    "text": "To improve model performance, we estimate class uncertainty and plot these pixel error metrics. Results below reveal highest uncertainty levels in classification of wetland and water areas.\n\n# Calculate the uncertainty cube\ns2_cube_uncert &lt;- sits_uncertainty(\n  cube = s2_cube_bayes,\n  type = \"margin\",\n  output_dir = \"./cubes/03_error/\",\n  memsize = 16,\n  multicores = 4\n)\n\nplot(s2_cube_uncert)\n\nAs expected, the places of highest uncertainty are those covered by surface water or associated with wetlands. These places are likely to be misclassified. For this reason, sits provides sits_uncertainty_sampling(), which takes the uncertainty cube as its input and produces a tibble with locations in WGS84 with high uncertainty (Camara et al., n.d.).\n\n# Find samples with high uncertainty\nnew_samples &lt;- sits_uncertainty_sampling(\n  uncert_cube = s2_cube_uncert,\n  n = 20,\n  min_uncert = 0.5,\n  sampling_window = 10\n  )\n\n# View the location of the samples\nsits_view(new_samples)"
  },
  {
    "objectID": "VT0007-deforestation-map.html#add-training-samples",
    "href": "VT0007-deforestation-map.html#add-training-samples",
    "title": "JNR Deforestation Risk Maps",
    "section": "",
    "text": "We can then use these points of high-uncertainty as new samples to add to our current training dataset. Once we identify their feature classes and relabel them correctly, we append them to derive an augmented samples_round_2.\n\n# Label the new samples\nnew_samples$label &lt;- \"Wetland\"\n\n# Obtain the time series from the regularized cube\nnew_samples_ts &lt;- sits_get_data(\n  cube = s2_reg_cube_ro,\n  samples = new_samples\n  )\n\n# Add new class to original samples\nsamples_round_2 &lt;- dplyr::bind_rows(\n  samples_4classes_3bands,\n  new_samples_ts\n  )\n\n# Train a RF model with the new sample set\nrfor_model_v2 &lt;- sits_train(\n  samples = samples_round_2,\n  ml_method = sits_rfor()\n  )\n\n# Classify the small area cube\ns2_cube_probs_v2 &lt;- sits_classify(\n  data = s2_reg_cube_ro,\n  ml_model = rfor_model_v2,\n  output_dir = \"./cubes/02_class/\",\n  version = \"v2\",\n  memsize = 16,\n  multicores = 4\n  )\n\n# Post-process the probability cube\ns2_cube_bayes_v2 &lt;- sits_smooth(\n  cube = s2_cube_probs_v2,\n  output_dir = \"./cubes/04_smooth/\",\n  version = \"v2\",\n  memsize = 16,\n  multicores = 4\n  )\n\n# Label the post-processed  probability cube\ns2_cube_label_v2 &lt;- sits_label_classification(\n  cube = s2_cube_bayes_v2,\n  output_dir = \"./cubes/05_tuned/\",\n  version = \"v2\",\n  memsize = 16,\n  multicores = 4\n  )\n\n# Plot the second version of the classified cube\nplot(s2_cube_label_v2)"
  },
  {
    "objectID": "VT0007-deforestation-map.html#remap-uncertainty",
    "href": "VT0007-deforestation-map.html#remap-uncertainty",
    "title": "JNR Deforestation Risk Maps",
    "section": "",
    "text": "# Calculate the uncertainty cube\ns2_cube_uncert_v2 &lt;- sits_uncertainty(\n  cube = s2_cube_bayes_v2,\n  type = \"margin\",\n  output_dir = \"./cubes/03_error/\",\n  version = \"v2\",\n  memsize = 16,\n  multicores = 4\n)\n\nplot(s2_cube_uncert_v2)"
  },
  {
    "objectID": "VT0007-deforestation-map.html#accuracy-assessment",
    "href": "VT0007-deforestation-map.html#accuracy-assessment",
    "title": "JNR Deforestation Risk Maps",
    "section": "",
    "text": "To select a validation subset of the map, sits recommends Cochran’s method for stratified random sampling (Cochran 1977). The method divides the population into homogeneous subgroups, or strata, and then applying random sampling within each stratum. Alternatively, ad-hoc parameterization is suggested as follows.\n\nro_sampling_design &lt;- sits_sampling_design(\n  cube = s2_cube_label_v2,\n  expected_ua = c(\n    \"Burned_Area\"       = 0.75,\n    \"Cleared_Area\"      = 0.70,\n    \"Forest\"            = 0.75,\n    \"Highly_Degraded\"   = 0.70,\n    \"Wetland\"           = 0.70\n  ),\n  alloc_options         = c(120, 100),\n  std_err               = 0.01,\n  rare_class_prop       = 0.1\n)\n# show sampling desing\nro_sampling_design"
  },
  {
    "objectID": "VT0007-deforestation-map.html#split-traintest-data",
    "href": "VT0007-deforestation-map.html#split-traintest-data",
    "title": "JNR Deforestation Risk Maps",
    "section": "",
    "text": "ro_samples_sf &lt;- sits_stratified_sampling(\n  cube                  = s2_cube_label_v2,\n  sampling_design       = ro_sampling_design,\n  alloc                 = \"alloc_120\",\n  multicores            = 4,\n  shp_file              = \"./samples/ro_samples.shp\"\n)\n\nsf::st_write(ro_samples_sf,\n  \"./samples/ro_samples.csv\",\n  layer_options = \"GEOMETRY=AS_XY\",\n  append = FALSE # TRUE if editing existing sample\n)"
  },
  {
    "objectID": "VT0007-deforestation-map.html#confusion-matrix",
    "href": "VT0007-deforestation-map.html#confusion-matrix",
    "title": "JNR Deforestation Risk Maps",
    "section": "",
    "text": "# Calculate accuracy according to Olofsson's method\narea_acc &lt;- sits_accuracy(s2_cube_label_v2,\n  validation = ro_samples_sf,\n  multicores = 4\n)\n# Print the area estimated accuracy\narea_acc\n\n# Print the confusion matrix\narea_acc$error_matrix"
  },
  {
    "objectID": "VT0007-deforestation-map.html#times-series-visualization",
    "href": "VT0007-deforestation-map.html#times-series-visualization",
    "title": "JNR Deforestation Risk Maps",
    "section": "",
    "text": "summary(as.data.frame(ro_samples_sf))"
  },
  {
    "objectID": "VT0007-deforestation-map.html#deforestation-binary-map",
    "href": "VT0007-deforestation-map.html#deforestation-binary-map",
    "title": "JNR Deforestation Risk Maps",
    "section": "Deforestation binary map",
    "text": "Deforestation binary map"
  },
  {
    "objectID": "VT0007-deforestation-map.html#deforestation-risk-map",
    "href": "VT0007-deforestation-map.html#deforestation-risk-map",
    "title": "JNR Deforestation Risk Maps",
    "section": "Deforestation risk map",
    "text": "Deforestation risk map"
  },
  {
    "objectID": "lib/python3.8/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "href": "lib/python3.8/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "title": "VT0007",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "href": "lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "title": "UAT for NbAgg backend.",
    "section": "",
    "text": "from imp import reload"
  },
  {
    "objectID": "lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-for-nbagg-backend.",
    "href": "lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-for-nbagg-backend.",
    "title": "UAT for NbAgg backend.",
    "section": "UAT for NbAgg backend.",
    "text": "UAT for NbAgg backend.\nThe first line simply reloads matplotlib, uses the nbagg backend and then reloads the backend, just to ensure we have the latest modification to the backend code. Note: The underlying JavaScript will not be updated by this process, so a refresh of the browser after clearing the output and saving is necessary to clear everything fully.\n\nimport matplotlib\nreload(matplotlib)\n\nmatplotlib.use('nbagg')\n\nimport matplotlib.backends.backend_nbagg\nreload(matplotlib.backends.backend_nbagg)\n\n\nUAT 1 - Simple figure creation using pyplot\nShould produce a figure window which is interactive with the pan and zoom buttons. (Do not press the close button, but any others may be used).\n\nimport matplotlib.backends.backend_webagg_core\nreload(matplotlib.backends.backend_webagg_core)\n\nimport matplotlib.pyplot as plt\nplt.interactive(False)\n\nfig1 = plt.figure()\nplt.plot(range(10))\n\nplt.show()\n\n\n\nUAT 2 - Creation of another figure, without the need to do plt.figure.\nAs above, a new figure should be created.\n\nplt.plot([3, 2, 1])\nplt.show()\n\n\n\nUAT 3 - Connection info\nThe printout should show that there are two figures which have active CommSockets, and no figures pending show.\n\nprint(matplotlib.backends.backend_nbagg.connection_info())\n\n\n\nUAT 4 - Closing figures\nClosing a specific figure instance should turn the figure into a plain image - the UI should have been removed. In this case, scroll back to the first figure and assert this is the case.\n\nplt.close(fig1)\nplt.close('all')\n\n\n\nUAT 5 - No show without plt.show in non-interactive mode\nSimply doing a plt.plot should not show a new figure, nor indeed update an existing one (easily verified in UAT 6). The output should simply be a list of Line2D instances.\n\nplt.plot(range(10))\n\n\n\nUAT 6 - Connection information\nWe just created a new figure, but didn’t show it. Connection info should no longer have “Figure 1” (as we closed it in UAT 4) and should have figure 2 and 3, with Figure 3 without any connections. There should be 1 figure pending.\n\nprint(matplotlib.backends.backend_nbagg.connection_info())\n\n\n\nUAT 7 - Show of previously created figure\nWe should be able to show a figure we’ve previously created. The following should produce two figure windows.\n\nplt.show()\nplt.figure()\nplt.plot(range(5))\nplt.show()\n\n\n\nUAT 8 - Interactive mode\nIn interactive mode, creating a line should result in a figure being shown.\n\nplt.interactive(True)\nplt.figure()\nplt.plot([3, 2, 1])\n\nSubsequent lines should be added to the existing figure, rather than creating a new one.\n\nplt.plot(range(3))\n\nCalling connection_info in interactive mode should not show any pending figures.\n\nprint(matplotlib.backends.backend_nbagg.connection_info())\n\nDisable interactive mode again.\n\nplt.interactive(False)\n\n\n\nUAT 9 - Multiple shows\nUnlike most of the other matplotlib backends, we may want to see a figure multiple times (with or without synchronisation between the views, though the former is not yet implemented). Assert that plt.gcf().canvas.manager.reshow() results in another figure window which is synchronised upon pan & zoom.\n\nplt.gcf().canvas.manager.reshow()\n\n\n\nUAT 10 - Saving notebook\nSaving the notebook (with CTRL+S or File-&gt;Save) should result in the saved notebook having static versions of the figures embedded within. The image should be the last update from user interaction and interactive plotting. (check by converting with ipython nbconvert &lt;notebook&gt;)\n\n\nUAT 11 - Creation of a new figure on second show\nCreate a figure, show it, then create a new axes and show it. The result should be a new figure.\nBUG: Sometimes this doesn’t work - not sure why (@pelson).\n\nfig = plt.figure()\nplt.axes()\nplt.show()\n\nplt.plot([1, 2, 3])\nplt.show()\n\n\n\nUAT 12 - OO interface\nShould produce a new figure and plot it.\n\nfrom matplotlib.backends.backend_nbagg import new_figure_manager,show\n\nmanager = new_figure_manager(1000)\nfig = manager.canvas.figure\nax = fig.add_subplot(1,1,1)\nax.plot([1,2,3])\nfig.show()"
  },
  {
    "objectID": "lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "href": "lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "title": "UAT for NbAgg backend.",
    "section": "UAT 13 - Animation",
    "text": "UAT 13 - Animation\nThe following should generate an animated line:\n\nimport matplotlib.animation as animation\nimport numpy as np\n\nfig, ax = plt.subplots()\n\nx = np.arange(0, 2*np.pi, 0.01)        # x-array\nline, = ax.plot(x, np.sin(x))\n\ndef animate(i):\n    line.set_ydata(np.sin(x+i/10.0))  # update the data\n    return line,\n\n#Init only required for blitting to give a clean slate.\ndef init():\n    line.set_ydata(np.ma.array(x, mask=True))\n    return line,\n\nani = animation.FuncAnimation(fig, animate, np.arange(1, 200), init_func=init,\n                              interval=100., blit=True)\nplt.show()\n\n\nUAT 14 - Keyboard shortcuts in IPython after close of figure\nAfter closing the previous figure (with the close button above the figure) the IPython keyboard shortcuts should still function.\n\n\nUAT 15 - Figure face colours\nThe nbagg honours all colours apart from that of the figure.patch. The two plots below should produce a figure with a red background. There should be no yellow figure.\n\nimport matplotlib\nmatplotlib.rcParams.update({'figure.facecolor': 'red',\n                            'savefig.facecolor': 'yellow'})\nplt.figure()\nplt.plot([3, 2, 1])\n\nplt.show()\n\n\n\nUAT 16 - Events\nPressing any keyboard key or mouse button (or scrolling) should cycle the line while the figure has focus. The figure should have focus by default when it is created and re-gain it by clicking on the canvas. Clicking anywhere outside of the figure should release focus, but moving the mouse out of the figure should not release focus.\n\nimport itertools\nfig, ax = plt.subplots()\nx = np.linspace(0,10,10000)\ny = np.sin(x)\nln, = ax.plot(x,y)\nevt = []\ncolors = iter(itertools.cycle(['r', 'g', 'b', 'k', 'c']))\ndef on_event(event):\n    if event.name.startswith('key'):\n        fig.suptitle('%s: %s' % (event.name, event.key))\n    elif event.name == 'scroll_event':\n        fig.suptitle('%s: %s' % (event.name, event.step))\n    else:\n        fig.suptitle('%s: %s' % (event.name, event.button))\n    evt.append(event)\n    ln.set_color(next(colors))\n    fig.canvas.draw()\n    fig.canvas.draw_idle()\n\nfig.canvas.mpl_connect('button_press_event', on_event)\nfig.canvas.mpl_connect('button_release_event', on_event)\nfig.canvas.mpl_connect('scroll_event', on_event)\nfig.canvas.mpl_connect('key_press_event', on_event)\nfig.canvas.mpl_connect('key_release_event', on_event)\n\nplt.show()\n\n\n\nUAT 17 - Timers\nSingle-shot timers follow a completely different code path in the nbagg backend than regular timers (such as those used in the animation example above.) The next set of tests ensures that both “regular” and “single-shot” timers work properly.\nThe following should show a simple clock that updates twice a second:\n\nimport time\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\n\ndef update(text):\n    text.set(text=time.ctime())\n    text.axes.figure.canvas.draw()\n    \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\ntimer.start()\nplt.show()\n\nHowever, the following should only update once and then stop:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center') \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\n\nplt.show()\n\nAnd the next two examples should never show any visible text at all:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\n\nUAT 18 - stopping figure when removed from DOM\nWhen the div that contains from the figure is removed from the DOM the figure should shut down it’s comm, and if the python-side figure has no more active comms, it should destroy the figure. Repeatedly running the cell below should always have the same figure number\n\nfig, ax = plt.subplots()\nax.plot(range(5))\nplt.show()\n\nRunning the cell below will re-show the figure. After this, re-running the cell above should result in a new figure number.\n\nfig.canvas.manager.reshow()\n\n\n\nUAT 19 - Blitting\nClicking on the figure should plot a green horizontal line moving up the axes.\n\nimport itertools\n\ncnt = itertools.count()\nbg = None\n\ndef onclick_handle(event):\n    \"\"\"Should draw elevating green line on each mouse click\"\"\"\n    global bg\n    if bg is None:\n        bg = ax.figure.canvas.copy_from_bbox(ax.bbox) \n    ax.figure.canvas.restore_region(bg)\n\n    cur_y = (next(cnt) % 10) * 0.1\n    ln.set_ydata([cur_y, cur_y])\n    ax.draw_artist(ln)\n    ax.figure.canvas.blit(ax.bbox)\n\nfig, ax = plt.subplots()\nax.plot([0, 1], [0, 1], 'r')\nln, = ax.plot([0, 1], [0, 0], 'g', animated=True)\nplt.show()\nax.figure.canvas.draw()\n\nax.figure.canvas.mpl_connect('button_press_event', onclick_handle)"
  },
  {
    "objectID": "lib/python3.8/site-packages/pandas-2.0.3.dist-info/AUTHORS.html",
    "href": "lib/python3.8/site-packages/pandas-2.0.3.dist-info/AUTHORS.html",
    "title": "About the Copyright Holders",
    "section": "",
    "text": "About the Copyright Holders\n\nCopyright (c) 2008-2011 AQR Capital Management, LLC\nAQR Capital Management began pandas development in 2008. Development was led by Wes McKinney. AQR released the source under this license in 2009.\nCopyright (c) 2011-2012, Lambda Foundry, Inc.\nWes is now an employee of Lambda Foundry, and remains the pandas project lead.\nCopyright (c) 2011-2012, PyData Development Team\nThe PyData Development Team is the collection of developers of the PyData project. This includes all of the PyData sub-projects, including pandas. The core team that coordinates development on GitHub can be found here: https://github.com/pydata.\n\nFull credits for pandas contributors can be found in the documentation.\n\n\nOur Copyright Policy\nPyData uses a shared copyright model. Each contributor maintains copyright over their contributions to PyData. However, it is important to note that these contributions are typically only changes to the repositories. Thus, the PyData source code, in its entirety, is not the copyright of any single person or institution. Instead, it is the collective copyright of the entire PyData Development Team. If individual contributors want to maintain a record of what changes/contributions they have specific copyright on, they should indicate their copyright in the commit message of the change when they commit the change to one of the PyData repositories.\nWith this in mind, the following banner should be used in any source code file to indicate the copyright and license terms:\n#-----------------------------------------------------------------------------\n# Copyright (c) 2012, PyData Development Team\n# All rights reserved.\n#\n# Distributed under the terms of the BSD Simplified License.\n#\n# The full license is in the LICENSE file, distributed with this software.\n#-----------------------------------------------------------------------------\nOther licenses can be found in the LICENSES directory.\n\n\nLicense\npandas is distributed under a 3-clause (“Simplified” or “New”) BSD license. Parts of NumPy, SciPy, numpydoc, bottleneck, which all have BSD-compatible licenses, are included. Their licenses follow the pandas license."
  },
  {
    "objectID": "lib/python3.8/site-packages/idna-3.10.dist-info/LICENSE.html",
    "href": "lib/python3.8/site-packages/idna-3.10.dist-info/LICENSE.html",
    "title": "VT0007",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "lib/python3.8/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "href": "lib/python3.8/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "title": "VT0007",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "lib/python3.8/site-packages/httpcore-1.0.7.dist-info/licenses/LICENSE.html",
    "href": "lib/python3.8/site-packages/httpcore-1.0.7.dist-info/licenses/LICENSE.html",
    "title": "VT0007",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "lib/python3.8/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "href": "lib/python3.8/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "title": "VT0007",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2024 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "rgee_CA.html",
    "href": "rgee_CA.html",
    "title": "RGEE CA",
    "section": "",
    "text": "This code will grab ndvi (normalized difference vegetation index), and lst (land surface temperature). The values from the images are then extracted for each ZCTA geometry from the data.\nThis is done for both Landsat 7 and Landsat 8 to give a time range from 2004-2020.\n\n#install.packages(\"rgee\")\n\n#these lines are specifically to get python environment named \"rgee\" working. if your setup works\n#feel free to comment these lines out. if your python environment has a different name, please replace the first argument in reticulate::use_condaenv.\n\n#in order for these lines to do their job, they must be the first command run on a fresh rstudio instance.\nlibrary(reticulate)\nreticulate::use_condaenv(\"rgee\", conda = \"auto\",required = TRUE)\n\n#in case your computer isn't allocating enough memory to run the code\nmemory.limit(64000)\n\nlibrary(rgee)\nlibrary(mapview)\nlibrary(exactextractr)\nlibrary(raster)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tigris)\nlibrary(leaflet)\nlibrary(sp)\nlibrary(rgeos)\nlibrary(geojsonio)\nlibrary(googledrive)\nlibrary(stars)\n\n#only needs to be run once total per computer/setup (in theory).\n#ee_install(py_env = \"rgee\")\n\nLoad in the data sets, before you initialize google earth engine\n\nprojection &lt;- \"+proj=utm +zone=10 +ellps=GRS80 +datum=NAD83 +units=ft +no_defs\"\n\ndata_ca_month &lt;- readRDS(\"G:/Shared drives/SFBI-Restricted/PHS/CA_month_ZCTAS_m.rds\") %&gt;% \n  select(ZIPCODE_5, geometry)\n#data_ca_year &lt;- readRDS(\"G:/Shared drives/SFBI-Restricted/PHS/CA_year_ZCTAS_m.rds\") %&gt;% \n#  select(ZIPCODE_5, geometry)\n#data_ca_season &lt;- readRDS(\"G:/Shared drives/SFBI-Restricted/PHS/CA_season_ZCTAS_m.rds\") %&gt;% \n#  select(ZIPCODE_5, geometry)\n\n#Merge the data\ndata_ca_merge &lt;- data_ca_month %&gt;% \n  rbind(data_ca_year) %&gt;% \n  rbind(data_ca_season) %&gt;% \n  ungroup() %&gt;%\n  st_as_sf() %&gt;%\n  as.data.frame() %&gt;%\n  st_as_sf() %&gt;%\n  unique()\n\nGrab the area of interest\n\n#06 is the code for CA\ncounties &lt;- ee$FeatureCollection(\"TIGER/2016/Counties\")\n myfilter &lt;- ee$Filter$inList(\n   opt_leftField = \"STATEFP\",\n   opt_rightValue = list(\n     \"06\"\n   )\n )\n\nca_counties &lt;- counties$filter(myfilter)\n\nCreating the cloud and water mask for Landsat 8 surface reflectance\n\ncloud_mask &lt;- function(raster_comp){\n  #this reassigns cloud shadow pixels to clear pixels\n  cloudShadowBitMask &lt;- bitwShiftL(1,3)\n  \n  #this reassigns cloud pixels to be clear\n  cloudsBitMask &lt;- bitwShiftL(1,5)\n  \n  #this reassigns water pixelx to clear pixels\n  waterBitMask &lt;- bitwShiftL(1,2)\n  \n  #select pixel_qa band and set it to a variable\n  qa &lt;- raster_comp$select('pixel_qa')\n  \n  #Multiplies the bits in the original pixel_qa band by the new reassigned cloud shadow transparent bits\n  mask &lt;- qa$bitwise_and(cloudShadowBitMask)$eq(0)$\n    And(qa$bitwise_and(cloudsBitMask)$eq(0))$\n    And(qa$bitwise_and(waterBitMask)$eq(0))\n  \n  #The cloud cover and cloud shadow pixels now have bit values of 1 = transparent\n  #Transparent pixels will not be include in the further analysis\n  raster_comp$updateMask(mask)\n}\n\nThe following code chunks is to generate various data frames for various years. In this case, 2014 to 2020.\n\n#landsat 8 covers the years 2014-2020, landsat 7 covers the years 2004-2013.\n#create a dataset of dates for 2014 to 2020 (Landsat 8) and a separate one of 2004 to 2013 (Landsat 7)\n\n#only one of the below time filters can be used at a time. both included for convenience\n\n#for filtering Landsat 8 data by years\nyears_all &lt;- data.frame(\n  years_start = c((seq(as.Date(\"2014-01-01\"), as.Date(\"2020-01-01\"), by = \"years\"))) %&gt;% \n    as.character(),\n  years_end =   c((seq(as.Date(\"2014-12-31\"), as.Date(\"2020-12-31\"), by = \"years\"))) %&gt;% \n    as.character(),\n  years = c(seq(\"2014\", \"2020\"))\n)\n\n# for filtering Landsat 8 data by months\nmonths_all &lt;- data.frame(\n  months_start = c((seq(as.Date(\"2014-01-01\"), as.Date(\"2020-12-01\"), by = \"months\"))) %&gt;% \n    as.character(),\n  months_end =   c((seq(as.Date(\"2014-01-31\"), as.Date(\"2020-12-31\"), by = \"months\"))) %&gt;% \n    as.character(),\n  months = c(seq(\"01\", \"12\")),\n  years = c(rep(2014:2020, each=12))\n)\n\n\n# these variables are used for chunking the dataframe into smaller pieces that don't violate\n# the memory limit of the ee_extract function.\nslice_sz = 200\nn_obsv = 1375 #n ZCTAs\nn_slice = floor(n_obsv/slice_sz)\n\n\n#this will grab values for ndvi by month\n# for (i in 5) {\nfor (i in 1:nrow(months_all)){\n   # for (j in length(years_end)){\n     ndvi_l8srt_combo &lt;- ee$ImageCollection(\"LANDSAT/LC08/C01/T1_SR\")$ #Grabbing landsat 8 surface reflectance data\n     filterBounds(ca_counties)$ #filtering within the bay =[-0rea counties\n     filterDate(months_all$months_start[i], months_all$months_end[i])$ #filter for the start and finish dates\n       \n     map(cloud_mask)$ #clear the clouds, cloud shadows, and water\n     median()$ #create a median image from the image collection\n     normalizedDifference(c('B5', 'B4'))$rename(\"ndvi\")\n\n     for (j in 1:n_slice) {\n      ee_selection = data_ca_merge[\"ZIPCODE_5\"]%&gt;%slice((1 + (j-1)*slice_sz):(j*slice_sz))\n      #print(paste0(\"grabbing entries \", (1+(j-1)*(slice_sz)), \" to \", j*slice_sz))\n       #extract the values\n      ndvi_ca_ &lt;- ee_extract(\n        x = ndvi_l8srt_combo,\n        y = ee_selection,\n        scale = 30,\n        fun = ee$Reducer$mean(),\n        sf = TRUE\n      )\n      #create unique dataframes to later bind\n      assign(paste0(\"ndvi_ca_\", months_all$years[i], \"-\", months_all$months[i], j, sep = \"\"), ndvi_ca_)\n\n      #Save! Especially if you have bad wifi!!!!!!\n      saveRDS(ndvi_ca_, paste0(\"ndvi_ca_\", months_all$years[i], \"-\", months_all$months[i], \"_chunk_\", j, \".rds\"))\n\n      #print for your sanity\n      #print(paste0(\"done with chunk \", j, \" of year \", i))\n     }\n     \n     ee_selection = data_ca_merge[\"ZIPCODE_5\"]%&gt;%slice(((n_slice*slice_sz)+1):n_obsv)\n     #print(paste0(\"grabbing entries \", (n_slice*slice_sz)+1, \" to \", n_obsv))\n     #last = data_ca_merge[\"ZIPCODE_5\"]%&gt;%slice(1201:1375)\n     #this is for processing the remainder chunk\n     ndvi_ca_ &lt;- ee_extract(\n        x = ndvi_l8srt_combo,\n        y = ee_selection,\n        scale = 30,\n        fun = ee$Reducer$mean(),\n        sf = TRUE\n      )\n      #create unique dataframes to later bind\n      assign(paste0(\"ndvi_ca_\", months_all$years[i], \"-\", months_all$months[i], (n_slice+1), sep = \"\"), ndvi_ca_)\n     \n      #Save! Especially if you have bad wifi!!!!!!\n      saveRDS(ndvi_ca_, paste0(\"ndvi_ca_\", months_all$years[i], \"-\", months_all$months[i], \"_chunk_\", (n_slice+1), \".rds\"))\n     \n    #print for your own sanity\n     print(paste0(\"done with year \",i, \" for ndvi for sat8\"))\n  \n}\n\n\nslice_sz = 100 #ee_extract can handle up to like 500 entries at a time seemingly\nn_obsv = 1375\nn_slice = floor(n_obsv/slice_sz)\n#Now doing this for land surface temperature\ncounties &lt;- ee$FeatureCollection(\"TIGER/2016/Counties\")\n\nfor (i in 1:nrow(months_all)){\n  if ((months_all_7$months[i] == 5)||(months_all_7$months[i] == 6)||(months_all_7$months[i] == 7)||(months_all_7$months[i] == 8)) {\n  med &lt;- ee$ImageCollection(\"LANDSAT/LC08/C01/T1_SR\")$ #Grabbing landsat 8 surface reflectance data\n  filterBounds(ca_counties)$ #filtering within the ca area counties\n  filterDate(months_all$months_start[i], months_all$months_end[i])$ #filter for the start and finish dates\n  map(cloud_mask)$ #clear the clouds, cloud shadows, and water\n  median()\n\n  ndvi &lt;- med$normalizedDifference(c('B5', 'B4'))$rename(\"ndvi\")\n\n  thermal &lt;- med$select('B10')$multiply(0.1)  \n    \n  for (j in 1:n_slice) {\n    cur_geo = data_ca_merge[\"geometry\"]%&gt;%slice((1 +(j-1)*slice_sz):(j*slice_sz))\n    cur_counties = sf_as_ee(cur_geo)\n \n    ndvi_min &lt;- ee$Number(ndvi$reduceRegion(\n      reducer = ee$Reducer$min(),\n      geometry = cur_counties,\n      scale =  30,\n      maxPixels = 1e9\n    )$values()$get(0))\n\n    ndvi_max &lt;- ee$Number(ndvi$reduceRegion(\n      reducer = ee$Reducer$max(),\n      geometry = cur_counties,\n      scale =  30,\n      maxPixels = 1e9\n    )$values()$get(0))\n\n    #fractional vegetation\n    fv &lt;- (ndvi$subtract(ndvi_min)$divide(ndvi_max$subtract(ndvi_min)))$pow(ee$Number(2))$rename('FV')\n\n    #Emissivity\n    a &lt;- ee$Number(0.004)\n    b &lt;- ee$Number(0.986)\n    EM &lt;- fv$multiply(a)$add(b)$rename('EMM')\n\n    #calculate land surface temperature (this is in celsius)\n    LST &lt;- thermal$expression(\n      '(Tb/(1 + (0.00115* (Tb / 1.438))*log(Ep)))-273.15',\n      opt_map = list(\n        'Tb'= thermal$select('B10'),\n        'Ep'= EM$select('EMM')\n      )\n    )$rename('LST')\n\n    LST &lt;- LST$expression( #convert to fahrenheit\n      '(temp * 9/5) + 32',\n      opt_map = list(\n        'temp'= LST$select('LST')\n      )\n    )$rename('LST')\n    ##### ITER BEGIN\n  \n    ee_selection = data_ca_merge[\"ZIPCODE_5\"]%&gt;%slice((1 +(j-1)*slice_sz):(j*slice_sz))\n    lst_ca &lt;- ee_extract(\n      x = LST,\n      y = ee_selection,\n      scale = 30,\n      fun = ee$Reducer$mean(),\n      sf = TRUE\n    )\n\n    #create unique dataframes to later bind\n    assign(paste0(\"lst_ca_\", months_all$years[i], \"-\", months_all$months[i], sep = \"\"), lst_ca)\n\n    #Save! Especially if you have bad wifi!!!!!!\n    saveRDS(lst_ca, paste0(\"lst_ca_\",months_all$years[i], \"-\", months_all$months[i], \"_chunk_\", j, \".rds\"))\n    #print for your sanity\n    print(paste0(\"done with chunk \", j, \" of year/month  \", i))\n  }\n\n  ee_selection = data_ca_merge[\"ZIPCODE_5\"]%&gt;%slice(((n_slice*slice_sz)+1):n_obsv)\n\n  lst_ca &lt;- ee_extract(\n    x = LST,\n    y = ee_selection,\n    scale = 30,\n    fun = ee$Reducer$mean(),\n    sf = TRUE\n  )\n  #create unique dataframes to later bind\n  assign(paste0(\"lst_ca_\",months_all$years[i], \"-\", months_all$months[i], (n_slice+1), sep = \"\"), lst_ca)\n\n  #Save! Especially if you have bad wifi!!!!!!\n  saveRDS(lst_ca, paste0(\"lst_ca_\",months_all$years[i], \"-\", months_all$months[i], \"_chunk_\", (n_slice+1), \".rds\"))\n\n  #print for your own sanity\n  print(paste0(\"done with year/month \", i, \" for LST for sat8\"))\n}\n}\n\nsame code, but for the timeframe covered by landsat7\n\n# once again, only one time filter can be used at a time. both included for user's convenience\n\n# for filtering Landsat 7 data by years\nyears_all_7 &lt;- data.frame(\n  years_start = c((seq(as.Date(\"2004-01-01\"), as.Date(\"2013-01-01\"), by = \"years\"))) %&gt;%\n    as.character(),\n  years_end =   c((seq(as.Date(\"2004-12-31\"), as.Date(\"2013-12-31\"), by = \"years\"))) %&gt;%\n    as.character(),\n  years = c(seq(\"2004\", \"2013\"))\n)\n\n# for filtering Landsat 7 data by months\nmonths_all_7 &lt;- data.frame(\n  months_start = c((seq(as.Date(\"2004-01-01\"), as.Date(\"2013-12-01\"), by = \"months\"))) %&gt;%\n    as.character(),\n  months_end = c((seq(as.Date(\"2004-01-31\"), as.Date(\"2013-12-31\"), by = \"months\"))) %&gt;%\n    as.character(),\n  months = c(seq(\"01\", \"12\")),\n  years = c(rep(2004:2013, each=12))\n)\n\n\n# i was having difficulty getting the LANDSAT7 data with a chunk size of 200 ZCTAs at a time, \n# so i had to shorten the chunk size to 100 to get the code to run.\nslice_sz = 100\nn_obsv = 1375\nn_slice = floor(n_obsv/slice_sz)\n\n\n#Now doing this for land surface temperature\ncounties &lt;- ee$FeatureCollection(\"TIGER/2016/Counties\")\n\nfor (i in 67:nrow(months_all_7)){\n#if ((months_all_7$months[i] == 5)||(months_all_7$months[i] == 6)||(months_all_7$months[i] == 7)||(months_all_7$months[i] == 8)) {\n  med &lt;- ee$ImageCollection(\"LANDSAT/LE07/C01/T1_SR\")$ #Grabbing landsat 8 surface reflectance data\n  filterBounds(ca_counties)$ #filtering within the ca area counties\n  filterDate(months_all_7$months_start[i], months_all_7$months_end[i])$ #filter for the start and finish dates\n  map(cloud_mask)$ #clear the clouds, cloud shadows, and water\n  median()\n\n  ndvi &lt;- med$normalizedDifference(c('B4', 'B3'))$rename(\"ndvi\")\n\n  thermal &lt;- med$select('B6')$multiply(0.1)  \n    \n  for (j in 1:n_slice) {\n    cur_geo = data_ca_merge[\"geometry\"]%&gt;%slice((1 +(j-1)*slice_sz):(j*slice_sz))\n    cur_counties = sf_as_ee(cur_geo)\n \n    ndvi_min &lt;- ee$Number(ndvi$reduceRegion(\n      reducer = ee$Reducer$min(),\n      geometry = cur_counties,\n      scale =  30,\n      maxPixels = 1e9\n    )$values()$get(0))\n\n    ndvi_max &lt;- ee$Number(ndvi$reduceRegion(\n      reducer = ee$Reducer$max(),\n      geometry = cur_counties,\n      scale =  30,\n      maxPixels = 1e9\n    )$values()$get(0))\n\n    #fractional vegetation\n    fv &lt;- (ndvi$subtract(ndvi_min)$divide(ndvi_max$subtract(ndvi_min)))$pow(ee$Number(2))$rename('FV')\n\n    #Emissivity\n    a &lt;- ee$Number(0.004)\n    b &lt;- ee$Number(0.986)\n    EM &lt;- fv$multiply(a)$add(b)$rename('EMM')\n\n    #calculate land surface temperature (this is in celsius)\n    LST &lt;- thermal$expression(\n      '(Tb/(1 + (0.00115* (Tb / 1.438))*log(Ep)))-273.15',\n      opt_map = list(\n        'Tb'= thermal$select('B6'),\n        'Ep'= EM$select('EMM')\n      )\n    )$rename('LST')\n\n    LST &lt;- LST$expression( #convert to fahrenheit\n      '(temp * 9/5) + 32',\n      opt_map = list(\n        'temp'= LST$select('LST')\n      )\n    )$rename('LST')\n     # slice iteration\n  \n    ee_selection = data_ca_merge[\"ZIPCODE_5\"]%&gt;%slice((1 +(j-1)*slice_sz):(j*slice_sz))\n    lst_ca &lt;- ee_extract(\n      x = LST,\n      y = ee_selection,\n      scale = 30,\n      fun = ee$Reducer$mean(),\n      sf = TRUE\n    )\n\n    #create unique dataframes to later bind\n    assign(paste0(\"lst_ca_\", months_all_7$years[i], \"-\", months_all_7$months[i], sep = \"\"), lst_ca)\n\n    #Save! Especially if you have bad wifi!!!!!!\n    saveRDS(lst_ca, paste0(\"lst_ca_\",months_all_7$years[i], \"-\", months_all_7$months[i], \"_chunk_\", j, \".rds\"))\n    #print for your sanity\n    print(paste0(\"done with chunk \", j, \" of year/month  \", i))\n  }\n\n  ee_selection = data_ca_merge[\"ZIPCODE_5\"]%&gt;%slice(((n_slice*slice_sz)+1):n_obsv)\n\n  lst_ca &lt;- ee_extract(\n    x = LST,\n    y = ee_selection,\n    scale = 30,\n    fun = ee$Reducer$mean(),\n    sf = TRUE\n  )\n  #create unique dataframes to later bind\n  assign(paste0(\"lst_ca_\",months_all_7$years[i], \"-\", months_all_7$months[i], (n_slice+1), sep = \"\"), lst_ca)\n\n  #Save! Especially if you have bad wifi!!!!!!\n  saveRDS(lst_ca, paste0(\"lst_ca_\",months_all_7$years[i], \"-\", months_all_7$months[i], \"_chunk_\", (n_slice+1), \".rds\"))\n\n  #print for your own sanity\n  print(paste0(\"done with year/month \", i, \" for LST for sat8\"))\n#}\n}\n\n\n# for (i in 1:nrow(years_all_7)){\n#   ndvi_l7srt_combo &lt;- ee$ImageCollection(\"LANDSAT/LE07/C01/T1_SR\")$ #Grabbing landsat 8 surface reflectance data\n#   filterBounds(ca_counties)$ #filtering within the bay area counties\n#   filterDate(years_all_7$years_start[i], years_all_7$years_end[i])$ #filter for the start and finish dates\n#   map(cloud_mask)$ #clear the clouds, cloud shadows, and water\n#   median()$ #create a median image from the image collection\n#   normalizedDifference(c('B4', 'B3'))$rename(\"ndvi\") #calculate ndvi based on the image's band 5 and band 4 and rename the band \"ndvi\"\n# \n#   \n#   # slice iteration\n#   for (j in 1:n_slice) {\n#     ndvi_ca &lt;- ee_extract(\n#       x = ndvi_l7srt_combo,\n#       y = data_ca_merge[\"ZIPCODE_5\"]%&gt;%slice((1 + (j-1)*slice_sz):(j*slice_sz)),\n#       scale = 30,\n#       fun = ee$Reducer$mean(),\n#       sf = TRUE\n#     )\n#   \n#     #create unique dataframes to later bind\n#     assign(paste0(\"ndvi_ca_\", years_all_7$years[i], j, sep = \"\"), ndvi_ca)\n#   \n#     #Save! Especially if you have bad wifi!!!!!!\n#     saveRDS(ndvi_ca_, paste0(\"ndvi_ca_\", years_all_7$years[i], \"_chunk_\", j, \".rds\"))\n#     #print for your sanity\n#     print(paste0(\"done with chunk \", j, \" of year \", i))\n#     \n#   }\n#   ndvi_ca &lt;- ee_extract(\n#       x = ndvi_l7srt_combo,\n#       y = data_ca_merge[\"ZIPCODE_5\"]%&gt;%slice(((n_slice*slice_sz)+1):n_obsv),\n#       scale = 30,\n#       fun = ee$Reducer$mean(),\n#       sf = TRUE\n#   )\n#   #create unique dataframes to later bind\n#   assign(paste0(\"ndvi_ca_\", years_all_7$years[i], (n_slice+1), sep = \"\"), ndvi_ca)\n#   \n#   #Save! Especially if you have bad wifi!!!!!!\n#   saveRDS(ndvi_ca_, paste0(\"ndvi_ca_\", years_all_7$years[i], \"_chunk_\", (n_slice+1), \".rds\"))\n#   \n#   #print for your own sanity\n#   print(paste0(\"done with year \", i, \" for ndvi for sat7\"))\n# }\n\ngrabbing maximum daily temperature from GridMet\n\n# for filtering gridmet data by years\n# currently not used by temp_gridmet, but is an easy swap to do.\n years_all_gridmet &lt;- data.frame(\n   years_start = c((seq(as.Date(\"2004-01-01\"), as.Date(\"2020-01-01\"), by = \"years\"))) %&gt;%\n     as.character(),\n   years_end =   c((seq(as.Date(\"2004-12-31\"), as.Date(\"2020-12-31\"), by = \"years\"))) %&gt;%\n     as.character(),\n   years = c(seq(\"2004\", \"2020\"))\n )\n\n# for filtering gridmet data by months\n# this filter is currently in use via temp_gridmet.\n months_all_gridmet &lt;- data.frame(\n   months_start = c((seq(as.Date(\"2015-01-01\"), as.Date(\"2020-12-01\"), by = \"months\"))) %&gt;%\n     as.character(),\n   months_end = c((seq(as.Date(\"2015-01-31\"), as.Date(\"2020-12-31\"), by = \"months\"))) %&gt;%\n     as.character(),\n   months = c(seq(\"01\", \"12\")),\n   years = c(rep(2015:2020, each=12))\n )\n\n\nnote: resolution is 4 km. This will grab the maximum temperature for each day in that month and take the median (maximum) value for that month.\n\n\nthis code could be modified to count days over a certain max temperature instead, and used to gather different temperature data.\n\n for (i in 1:nrow(months_all_gridmet)){\n   gridmet&lt;- ee$ImageCollection(\"IDAHO_EPSCOR/GRIDMET\")$ #grabbing maximum relative temperature from GridMET\n   filterBounds(ca_counties)$ #filtering within the bay area counties\n   filterDate(months_all_gridmet$months_start[i], months_all_gridmet$months_end[i])$\n   select(\"tmmx\")$\n   median()\n\n   temp &lt;- gridmet$expression(\n     '(temper - 273.15) * 9/5 + 32',\n     opt_map = list(\n       'temper'= gridmet$select('tmmx')\n     )\n   )$rename('temp')\n\n    # slice iteration\n    for (j in 1:n_slice) {\n     ee_selection = data_ca_merge[\"ZIPCODE_5\"]%&gt;%slice((1 + (j-1)*slice_sz):(j*slice_sz))\n\n     max_temp_ca &lt;- ee_extract(\n       x = temp,\n       y = ee_selection,\n       scale = 4000,\n       fun = ee$Reducer$mean(),\n       sf = TRUE\n     )\n\n     #create unique dataframes to later bind\n     assign(paste0(\"max_temp_ca_\",months_all_gridmet$years[i], \"-\", months_all_gridmet$months[i], j, sep = \"\"), max_temp_ca)\n\n     #Save! Especially if you have bad wifi!!!!!!\n     saveRDS(max_temp_ca, paste0(\"max_temp_ca_\",months_all_gridmet$years[i], \"-\", months_all_gridmet$months[i], \"_chunk_\", j, \".rds\"))\n     #print for your sanity\n     print(paste0(\"done with chunk \", j, \" of year \", i))\n    }\n\n   ee_selection = data_ca_merge[\"ZIPCODE_5\"]%&gt;%slice(((n_slice*slice_sz)+1):n_obsv)\n\n   max_temp_ca &lt;- ee_extract(\n       x = temp,\n       y = ee_selection,\n       scale = 4000,\n       fun = ee$Reducer$mean(),\n       sf = TRUE\n     )\n   #create unique dataframes to later bind\n   assign(paste0(\"max_temp_ca_\",  months_all_gridmet$years[i], \"-\", months_all_gridmet$months[i], (n_slice+1), sep = \"\"), max_temp_ca)\n\n   #Save! Especially if you have bad wifi!!!!!!\n   saveRDS(max_temp_ca, paste0(\"max_temp_ca_\", months_all_gridmet$years[i], \"-\", months_all_gridmet$months[i], \"_chunk_\", (n_slice+1), \".rds\"))\n   #print for your own sanity\n   print(paste0(\"done with month \", i, \" for max temp for gridmet\"))\n }\n\n\n\nnote: all data binding from this document is in data_binding.Rmd."
  },
  {
    "objectID": "VT0007-deforestation-map.html#environment-setup",
    "href": "VT0007-deforestation-map.html#environment-setup",
    "title": "JNR Deforestation Risk Maps",
    "section": "",
    "text": "Build/restore virtual environment: Python -&gt; R\nTo avoid issues with IDE settings, it is recommended to run the following virtual environment functions from an terminal external to RStudio or VScode. To update an previously loaded environment, simply run pip3 install -r requirements.txt in any terminal from the trunk directory.\n\n# create virtual environment \npython3 -m venv working_director_name\n\n# activate environment's python\nsource working_director_name/bin/activate\n\n# check python activation\npython3\nimport sys \nprint(sys.executable)\nquit()\n\n# restore environment of cloned repo\npython3 pip install -r requirements.txt\n\n# install packages manually\npython3 -m pip install numpy jupyter earthengine-api\n\n# save index of loaded packages\npython3 -m pip freeze &gt; requirements.txt\n\nAssign rgee kernel, gcs directory & credentials\n\nlibrary(rgee)\nlibrary(reticulate)\nlibrary(googledrive)\nlibrary(googleCloudStorageR)\n\nreticulate::use_python(\"./bin/python3\")\nreticulate::py_run_string(\"import ee; ee.Initialize()\")\nrgee::ee_install_set_pyenv(py_path = \"./bin/python3\", py_env = \"./\")\nrgee::ee_path = path.expand(\"/home/seamus/.config/earthengine/seamusrobertmurphy/credentials\")\nee_Initialize(user = \"seamusrobertmurphy\", gcs = T, drive = T)\n#ee_install()\n\n################################################\n# Assign the SaK & user for interactive web renders\nSaK_file = \"/home/seamus/Repos/api-keys/SaK_rgee.json\" \nee_utils_sak_copy(sakfile =  SaK_file, users = \"seamusrobertmurphy\")\n\n# Confirm project_id & bucket\nproject_id &lt;- ee_get_earthengine_path() %&gt;% \n  list.files(., \"\\\\.json$\", full.names = TRUE) %&gt;% \n  jsonlite::read_json() %&gt;% \n  '$'(project_id) \n#googleCloudStorageR::gcs_create_bucket(\"deforisk_bucket_1\", projectId = project_id)\n\n# Validate SaK credentials\nee_utils_sak_validate(\n    sakfile = SaK_file,\n    bucket = \"deforisk_bucket_1\",\n    quiet = F \n )\n\nJurisdictional boundaries\n\n# assign master crs\ncrs_master    = sf::st_crs(\"epsg:4326\")\n# derive aoi windows\naoi_country   = geodata::gadm(country=\"GUY\", level=0, path=tempdir()) |&gt;\n  sf::st_as_sf() |&gt; sf::st_cast() |&gt; sf::st_transform(crs_master)\n\naoi_states    = geodata::gadm(country=\"GUY\", level=1, path=tempdir()) |&gt;\n  sf::st_as_sf() |&gt; sf::st_cast() |&gt; sf::st_transform(crs_master) |&gt;\n  dplyr::rename(State = NAME_1)\n\naoi_target    = dplyr::filter(aoi_states, State == \"Barima-Waini\") \naoi_target_ee = rgee::sf_as_ee(aoi_target)\n\nEarth Engine client library not initialized. Run `ee.Initialize()`\n\n# visualize\ntmap::tmap_mode(\"view\")\ntmap::tm_shape(aoi_states) + tmap::tm_borders(col = \"white\", lwd = 0.5) +\n  tmap::tm_text(\"State\", col = \"white\", size = 1, alpha = 0.3, just = \"bottom\") +\n  tmap::tm_shape(aoi_country) + tmap::tm_borders(col = \"white\", lwd = 1) +\n  tmap::tm_shape(aoi_target) + tmap::tm_borders(col = \"red\", lwd = 2) +\n  tmap::tm_text(\"State\", col = \"red\", size = 1.3) +\n  tmap::tm_basemap(\"Esri.WorldImagery\")"
  },
  {
    "objectID": "VT0007-deforestation-map.html#assemble-hrp-data-cube",
    "href": "VT0007-deforestation-map.html#assemble-hrp-data-cube",
    "title": "JNR Deforestation Risk Maps",
    "section": "",
    "text": "We assemble a raster data cube representing a ten year historical reference period (HRP) between 2014-01-01 and 2024-12-31 for the state of Barina Waini, Guyana. Masking is applied to cloud, shadow and water surfaces with median normalization using a cloudless pixel ranking.\n\ncube_terra = terra::rast(\"./cubes/mosaic/LANDSAT-C2-L2_OLI_TILE_BAND_2014-01-11.tif\")\nnames(cube_terra)\n\ncube_2014 = sits_cube(\n  source     = \"MPC\",\n  collection = \"LANDSAT-C2-L2\",\n  data_dir   = here::here(\"cubes\", \"mosaic\"),\n  bands      = c(\"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B10\", \"NDVI\"),\n  version    = \"mosaic\"\n)\n\n# 2014 -------------------\n# cloud-assemble data cube\ncube_raw_2014 = sits::sits_cube(\n  source      = \"MPC\",\n  collection  = \"LANDSAT-C2-L2\",\n  bands       = c(\"RED\", \"GREEN\", \"BLUE\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n  roi         = aoi_target,\n  start_date  = as.Date(\"2014-01-01\"),\n  end_date    = as.Date(\"2014-01-15\"),\n  progress    = T\n  )\n\n# regularize data cube\ncube_reg_2014 = sits::sits_regularize(\n  cube        = cube_raw_2014,\n  roi         = aoi_target,\n  res         = 60,\n  period      = \"P180D\",\n  output_dir  = here::here(\"cubes\", \"reg\", \"2014\"),\n  memsize     = 16,\n  multicores  = 8,\n  progress    = T\n  )\n\n# 2019 -------------------\n# cloud-assemble data cube\ncube_raw_2019 = sits::sits_cube(\n  source      = \"MPC\",\n  collection  = \"LANDSAT-C2-L2\",\n  bands       = c(\"RED\", \"GREEN\", \"BLUE\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n  roi         = aoi_target,\n  start_date  = as.Date(\"2019-01-01\"),\n  end_date    = as.Date(\"2019-07-01\"),\n  progress    = T\n  )\n\n# regularize data cube\ncube_reg_2019 = sits::sits_regularize(\n  cube        = cube_raw_2019,\n  roi         = aoi_target,\n  res         = 60,\n  period      = \"P180D\",\n  output_dir  = here::here(\"cubes\", \"reg\", \"2019\"),\n  memsize     = 16,\n  multicores  = 8,\n  progress    = T\n  )\n\n# 2024 -------------------\n# cloud-assemble data cube\ncube_raw_2024 = sits::sits_cube(\n  source      = \"MPC\",\n  collection  = \"LANDSAT-C2-L2\",\n  bands       = c(\"RED\", \"GREEN\", \"BLUE\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n  roi         = aoi_target,\n  start_date  = as.Date(\"2024-01-01\"),\n  end_date    = as.Date(\"2024-07-01\"),\n  progress    = T\n  )\n\n# regularize data cube\ncube_reg_2024 = sits::sits_regularize(\n  cube        = cube_raw_2024,\n  roi         = aoi_target,\n  res         = 60,\n  period      = \"P180D\",\n  output_dir  = here::here(\"cubes\", \"reg\", \"2024\"),\n  memsize     = 16,\n  multicores  = 8,\n  progress    = T\n  )\n\n# plot cube timelines\nsits_timeline(cube_reg_2014)\nsits_timeline(cube_reg_2019)\nsits_timeline(cube_reg_2024)\nplot(cube_reg_2014,\n  red         = \"RED\",\n  green       = \"GREEN\",\n  blue        = \"BLUE\",\n  date        = \"2014-01-03\"\n  )\n\nplot(cube_reg_2019,\n  red         = \"RED\",\n  green       = \"GREEN\",\n  blue        = \"BLUE\",\n  date        = \"2019-01-08\"\n  )\n\nplot(cube_reg_2024,\n  red         = \"RED\",\n  green       = \"GREEN\",\n  blue        = \"BLUE\",\n  date        = \"2024-01-07\"\n  )\n\n\n\n\n\nWe import the GLanCE training dataset of annual times series points that includes 7 land cover classes (Figure 2; (Woodcock et al., n.d.)). Training samples are fitted to a Random Forest model and post-processed with a Bayesian smoothing and then evaluated using confusion matrix. The classifier is then calibrated by mapping pixel uncertainty, adding new samples in areas of high uncertainty, reclassifying with improved samples and re-evaluated using confusion matrix.\n\n\n\nFigure 2: Land cover classes included in the GLanCE Level 1 classification scheme (Woodcock et al 2022)\n\n\n\n# Extract training set from gee to drive & import: https://gee-community-catalog.org/projects/glance_training/?h=training \nglance_training_url = \"https://drive.google.com/file/d/1CgBP2J2OdOhmOiVS4hGibLEMyVLTe1_P/view?usp=drive_link\"\n# file_name = \"glance_training.csv\"\n# download.file(url = url, path = here::here(\"training\"), destfile = file_name)\nglance_training = read.csv(here::here(\"training\", \"glance_training.csv\"))\n\ndata(\"samples_prodes_4classes\")\n# Select the same three bands used in the data cube\nsamples_4classes_3bands &lt;- sits_select(\n  data = samples_prodes_4classes,\n  bands = c(\"B02\", \"B8A\", \"B11\")\n  )\n\n# Train a random forest model\nrfor_model &lt;- sits_train(\n  samples = samples_4classes_3bands,\n  ml_method = sits_rfor()\n  )\n\n# Classify the small area cube\ns2_cube_probs &lt;- sits_classify(\n  data = s2_reg_cube_ro,\n  ml_model = rfor_model,\n  output_dir = \"./cubes/02_class/\",\n  memsize = 15,\n  multicores = 5\n  )\n\n# Post-process the probability cube\ns2_cube_bayes &lt;- sits_smooth(\n  cube = s2_cube_probs,\n  output_dir = \"./cubes/02_class/\",\n  memsize = 16,\n  multicores = 4\n  )\n\n# Label the post-processed  probability cube\ns2_cube_label &lt;- sits_label_classification(\n  cube = s2_cube_bayes,\n  output_dir = \"./cubes/02_class/\",\n  memsize = 16,\n  multicores = 4\n  )\n\nplot(s2_cube_label)"
  },
  {
    "objectID": "VT0007_data_preprocessing.html",
    "href": "VT0007_data_preprocessing.html",
    "title": "VT0007",
    "section": "",
    "text": "Environment setup\n\n!pip install leafmap geemap==0.16.4 geopandas numpy scikit-learn\n#import geemap.tools as geemap_tools\nimport ee, json, geemap, ipyleaflet, os, numpy\nfrom google.colab import drive\nfrom google.colab import files\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n!drive.mount('/content/drive')\n\n\n\nActivate Earth Engine\n\n!earthengine authenticate\n#!ee.Authenticate() # now deprecated in certain Colab environments\nee.Initialize(project = \"murphys-deforisk\")\n\n\n\nJurisidictional boundaries\n\ncountry = ee.FeatureCollection('FAO/GAUL/2015/level1').filter(\n    ee.Filter.equals(\"ADM0_NAME\", \"Guyana\"))\nstates_all = country.aggregate_array('ADM1_NAME').distinct().getInfo()\nstates_all\n\n['Barima Waini (region N°1)',\n 'Cuyuni/mazaruni (region N°7)',\n 'Demerara Mahaica (region N°4)',\n 'East Berbice/corentyne (region N°6)',\n 'Essequibo Islands/west Demerara (region N°3)',\n 'Mahaica Berbice (region N°5)',\n 'Pomeroon/supenaam (region N°2',\n 'Potaro/siparuni (region N°8)',\n 'Upper Demerara/berbice (region N°10)',\n 'Upper Takutu/upper Essequibo (region N°9)']\n\n\n\nstate =  ee.FeatureCollection('FAO/GAUL/2015/level1').filter(\n    ee.Filter.equals('ADM1_NAME', \"Barima Waini (region N°1)\"))\nred = {\"color\": \"red\", \"width\": 2, \"lineType\": \"solid\", \"fillColor\": \"00000000\"}\nwhite = {\"color\": \"white\", \"width\": 1, \"lineType\": \"solid\", \"fillColor\": \"00000000\"}\ncountry_label = ee.FeatureCollection([ee.Feature(\n    country.geometry().centroid(), {'country_name': country.first().get(\"ADM0_NAME\").getInfo()})])\n\nMap = geemap.Map()\nMap.centerObject(country, 6)\nMap.add_basemap('Esri.WorldImagery')\nMap.addLayer(country.style(**white), {}, \"Guyana\")\nMap.addLayer(state.style(**red), {}, \"Barima Waini (region N°1)\")\nMap.add_labels(state,\"ADM1_NAME\",font_size=\"9pt\",font_color=\"red\",font_family=\"arial\",font_weight=\"bold\",)\nMap.add_labels(country_label, \"country_name\", font_size=\"12pt\", font_color=\"white\", font_family=\"arial\",)\nMap\n\n\n\n\nThe ImageCollection Landsat 8 Surface Reflectance Tier 1 have been atmospherically corrected using LaSRC and includes a cloud, shadow, water and snow mask produced using CFMASK, as well as a per-pixel saturation mask.\n\n# derive masking, scaling, and ndvi function\ndef maskL8sr(image):\n    qaMask = image.select('QA_PIXEL').bitwiseAnd(int('11111', 2)).eq(0)\n    saturationMask = image.select('QA_RADSAT').eq(0)\n    opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2)\n    thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0)\n    ndvi = image.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI').toFloat()\n    image = image.addBands(opticalBands, None, True) \\\n                 .addBands(thermalBands, None, True) \\\n                 .addBands(ndvi)\n    return image.select(\n        ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'NDVI'],\n        ['BLUE', 'GREEN', 'RED', 'NIR08', 'SWIR16', 'SWIR22', 'NDVI']\n    ).updateMask(qaMask).updateMask(saturationMask)\n\n# create collections for 2014 and 2024\ncollection_2014 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n                    .filterDate('2014-01-01', '2014-12-31') \\\n                    .filterBounds(state) \\\n                    .map(maskL8sr)\n\ncollection_2024 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n                    .filterDate('2024-01-01', '2024-12-31') \\\n                    .filterBounds(state) \\\n                    .map(maskL8sr)\n\n# median composites for 2014 and 2024\ncomposite_2014 = collection_2014.select(['BLUE', 'GREEN', 'RED', 'NIR08', 'SWIR16', 'SWIR22', 'NDVI']).median().clip(state).toFloat()\ncomposite_2024 = collection_2024.select(['BLUE', 'GREEN', 'RED', 'NIR08', 'SWIR16', 'SWIR22', 'NDVI']).median().clip(state).toFloat()\n\n\n# visualization\nndviVis = {'min': 0.2, 'max': 0.8, 'palette': ['red', 'yellow', 'green']}\nMap = geemap.Map()\nMap.centerObject(state, 8)\nMap.addLayer(composite_2014.select('NDVI'), ndviVis, 'NDVI 2014')\nMap.addLayer(composite_2024.select('NDVI'), ndviVis, 'NDVI 2024')\nMap.addLayer(state, {}, 'Area of Interest')\nMap.addLayerControl()\nMap\n\n\n\n\n\n# confirm dates, scene IDs, band names of images\nfirstImage_2014 = collection_2014.first()\nsceneId_2014 = firstImage_2014.get('system:index').getInfo()\nprint(f\"Scene ID for collection_2014: {sceneId_2014}\")\n\nfirstImage_2024 = collection_2024.first()\nsceneId_2024 = firstImage_2024.get('system:index').getInfo()\nprint(f\"Scene ID for collection_2024: {sceneId_2024}\")\n\nbandNames_2014 = composite_2014.bandNames().getInfo()\nprint(f\"Band names: {bandNames_2014}\")\n\nbandNames_2024 = composite_2024.bandNames().getInfo()\nprint(f\"Band names: {bandNames_2024}\")\n\nScene ID for collection_2014: LC08_231055_20140111\nScene ID for collection_2024: LC08_231055_20240107\nBand names: ['BLUE', 'GREEN', 'RED', 'NIR08', 'SWIR16', 'SWIR22', 'NDVI']\nBand names: ['BLUE', 'GREEN', 'RED', 'NIR08', 'SWIR16', 'SWIR22', 'NDVI']\n\n\n\nfrom datetime import datetime\n\n# extract pathrow and date from scene ID\ndef get_pathrow_date(image_collection):\n  first_image = image_collection.first()\n  scene_id = first_image.get('system:index').getInfo()\n  parts = scene_id.split('_')\n  pathrow = parts[1]\n  date_str = parts[2]\n  date_obj = datetime.strptime(date_str, '%Y%m%d')\n  date = date_obj.strftime('%Y-%m-%d')\n  return pathrow, date\n\n# define export parameters\ndef define_export_params(image, pathrow, date, band_name):\n  return {\n    'image': image.select(band_name),\n    'description': f'composite_{date}_{band_name}_export',\n    'bucket': 'deforisk_bucket_1',\n    'fileNamePrefix': f'LANDSAT_TM-ETM-OLI_{pathrow}_{band_name}_{date}',\n    'scale': 30,\n    'region': state.geometry(),\n    'maxPixels': 1e13,\n    'fileFormat': 'GeoTIFF',\n    'formatOptions': {'cloudOptimized': True}\n  }\n\n# get pathrow and date for each collection\npathrow_2014, date_2014 = get_pathrow_date(collection_2014)\npathrow_2024, date_2024 = get_pathrow_date(collection_2024)\n\n# get band names\nbandNames_2014 = composite_2014.bandNames().getInfo()\nbandNames_2024 = composite_2024.bandNames().getInfo()\n\n# export to cloud bucket\nfor band_name in bandNames_2014:\n    export_params_2014 = define_export_params(composite_2014, pathrow_2014, date_2014, band_name)\n    task_2014 = ee.batch.Export.image.toCloudStorage(**export_params_2014)\n    task_2014.start()\n    print(f\"Exporting 2014 image for band {band_name}. Task ID: {task_2014.id}\")\n\nfor band_name in bandNames_2024:\n    export_params_2024 = define_export_params(composite_2024, pathrow_2024, date_2024, band_name)\n    task_2024 = ee.batch.Export.image.toCloudStorage(**export_params_2024)\n    task_2024.start()\n    print(f\"Exporting 2024 image for band {band_name}. Task ID: {task_2024.id}\")\n\n# export full stack to drive\n#export_params_drive = {\n#    'image': composite_2014,\n#    'description': 'export_2014',\n#    'folder': 'VT0007-deforestation-map',\n#    'fileNamePrefix': 'LANDSAT-C2-L2_OLI_TILE_BAND_2014-11-11',\n#    'scale': 30,\n#    'region': state.geometry(),\n#    'maxPixels': 1e13,\n#    'fileFormat': 'GeoTIFF',\n#    'formatOptions': {'cloudOptimized': True}\n#}\n\n#task = ee.batch.Export.image.toDrive(**export_params_drive)\n#task.start()\n#print(f\"Exporting image. Task ID: {task.id}\")\n\nExporting 2014 image for band BLUE. Task ID: 6EA3PHOOJSSPSK62ZG73RM6G\nExporting 2014 image for band GREEN. Task ID: GCFMAH3UGO44UC55YLVZLW6C\nExporting 2014 image for band RED. Task ID: UPO5UJ4T3CLW6C6BFKISD3DF\nExporting 2014 image for band NIR08. Task ID: VSW7NGIQCALO7OD72OVMRVYQ\nExporting 2014 image for band SWIR16. Task ID: C5QAPB24TKTAS7EG6MIHC3GF\nExporting 2014 image for band SWIR22. Task ID: SUDF3XE45KNXJYIHUV4SCROO\nExporting 2014 image for band NDVI. Task ID: R2YUC2SMNLERTO2BPLCS74WE\nExporting 2024 image for band BLUE. Task ID: VQ5B74MLOJ6ZKLN3BOTIXAAC\nExporting 2024 image for band GREEN. Task ID: Y4JZGRXOAEAIIXOVR5I7ID3W\nExporting 2024 image for band RED. Task ID: B676JUQXPFLDD2ICYNLD54IC\nExporting 2024 image for band NIR08. Task ID: JJVTBGXY7TAHQL3IUU4ZFWCC\nExporting 2024 image for band SWIR16. Task ID: 574U4KUEBBERV6DJDFITLTYW\nExporting 2024 image for band SWIR22. Task ID: C7HFJWINW7OXEYYTOPTET3LB\nExporting 2024 image for band NDVI. Task ID: 3JK3BNYWQURBZ4GCUXG4YBXE"
  }
]